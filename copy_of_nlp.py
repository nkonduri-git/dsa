# -*- coding: utf-8 -*-
"""Copy of NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100hSm55lMs2YWYEnMT_849y-EyK7tiLS
"""

!pip install nlp

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random


def show_history(h):
    epochs_trained = len(h.history['loss'])
    plt.figure(figsize=(16, 6))

    plt.subplot(1, 2, 1)
    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')
    plt.ylim([0., 1.])
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


def show_confusion_matrix(y_true, y_pred, classes):
    from sklearn.metrics import confusion_matrix

    cm = confusion_matrix(y_true, y_pred, normalize='true')

    plt.figure(figsize=(8, 8))
    sp = plt.subplot(1, 1, 1)
    ctx = sp.matshow(cm)
    plt.xticks(list(range(0, 6)), labels=classes)
    plt.yticks(list(range(0, 6)), labels=classes)
    plt.colorbar(ctx)
    plt.show()


print('Using TensorFlow version', tf.__version__)

import pandas as pd

df = pd.read_pickle("merged_training.pkl")

df

from datasets import load_dataset



dataset = load_dataset('emotion')

dataset

train=dataset['train']
val=dataset['validation']
test=dataset['test']

# Re-define the get_tweet function
def get_tweet(data):
    tweets = [x['text'] for x in data]
    labels = [x['label'] for x in data]
    return tweets, labels

# Re-define classes
classes = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

# Create class mappings
class_to_index = dict((c, i) for i, c in enumerate(classes))
index_to_class = dict((v, k) for k, v in class_to_index.items())

# Function to convert label names to indices
names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])

print("Functions and mappings defined successfully!")
print(f"Classes: {classes}")
print(f"Class to index mapping: {class_to_index}")

tweets,labels=get_tweet(train)

tweets[0],labels[0]

#tokenization each word one number must be assigned so that it can be used in ml model in general every unique 1 token given but we can also adjust such that common words 1 token
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer(num_words=10000,oov_token="<OOV>")
tokenizer.fit_on_texts(tweets)
tokenizer.texts_to_sequences([tweets[0]])

lengths=[len(t.split(' ')) for t in tweets ]
plt.hist(lengths,bins=len(set(lengths)))
plt.show()#showing the most common features

maxlen=50
from tensorflow.keras.preprocessing.sequence import pad_sequences

def get_sequence(tokenizer,tweets):
  sequences=tokenizer.texts_to_sequences(tweets)
  paddes=pad_sequences(sequences,truncating='post',padding='post',maxlen=maxlen)
  return paddes
#every input exactly same size every tweet will of length 50

padded_train_seq=get_sequence(tokenizer,tweets)
padded_train_seq[0]

#preparing the labels
classes=set(labels)
print(classes)

# Fix: Use 'labels' (plural) and set bins to 6 (for 6 emotion classes)
plt.hist(labels, bins=6)
plt.xlabel('Emotion Label')
plt.ylabel('Frequency')
plt.title('Distribution of Emotions in Training Data')
plt.xticks(range(6), classes, rotation=45)
plt.show()

# First, define the classes with emotion names
classes = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

# Now create the mappings
class_to_index = dict((c, i) for i, c in enumerate(classes))
index_to_class = dict((v, k) for k, v in class_to_index.items())

print("class_to_index:", class_to_index)
print("index_to_class:", index_to_class)

names_to_ids=lambda labels:np.array([class_to_index.get(x)for x in labels])
train_labels=names_to_ids(labels)
print(train_labels[0])

#model
model=tf.keras.models.Sequential([
    tf.keras.layers.Embedding(10000,16,input_length=maxlen),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20,return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
    tf.keras.layers.Dense(6,activation='softmax')


])
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

# Define classes as a list
classes = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

# Prepare validation data
val_tweets, val_labels_raw = get_tweet(val)
val_seq = get_sequence(tokenizer, val_tweets)

# Labels from the 'emotion' dataset are already integers (0-5)
# So just convert to numpy array, no mapping needed
val_labels = np.array(val_labels_raw)

print(f"Validation tweets: {len(val_tweets)}")
print(f"Validation sequences shape: {val_seq.shape}")
print(f"Validation labels shape: {val_labels.shape}")
print(f"Sample validation tweet: {val_tweets[0]}")
print(f"Sample validation label: {val_labels[0]} ({classes[val_labels[0]]})")

val_labels,val_tweets

# For training data
tweets, labels_raw = get_tweet(train)
padded_train_seq = get_sequence(tokenizer, tweets)
train_labels = np.array(labels_raw)  # Just convert to array, labels are already integers

print(f"Training sequences shape: {padded_train_seq.shape}")
print(f"Training labels shape: {train_labels.shape}")

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(10000, 16, input_length=maxlen),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
    tf.keras.layers.Dense(6, activation='softmax')
])

# Compile the model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Display model architecture
model.summary()

# Train the model
epochs = 20
batch_size = 32

h = model.fit(
    padded_train_seq,
    train_labels,
    validation_data=(val_seq, val_labels),
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

# Show training history
show_history(h)

# Prepare test data
test_tweets, test_labels_raw = get_tweet(test)
test_seq = get_sequence(tokenizer, test_tweets)
test_labels = np.array(test_labels_raw)

print(f"Test sequences shape: {test_seq.shape}")
print(f"Test labels shape: {test_labels.shape}")

# Evaluate model
test_loss, test_accuracy = model.evaluate(test_seq, test_labels, verbose=1)
print(f'\n{"="*50}')
print(f'Test Accuracy: {test_accuracy*100:.2f}%')
print(f'Test Loss: {test_loss:.4f}')
print(f'{"="*50}')

# Make predictions
predictions = model.predict(test_seq)
predicted_labels = np.argmax(predictions, axis=1)

# Show confusion matrix
show_confusion_matrix(test_labels, predicted_labels, classes)

from sklearn.metrics import classification_report

print("\nClassification Report:")
print("="*60)
print(classification_report(test_labels, predicted_labels, target_names=classes))

# Function to predict sentiment of new tweets
def predict_sentiment(text):
    sequence = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(sequence, truncating='post', padding='post', maxlen=maxlen)
    prediction = model.predict(padded, verbose=0)
    predicted_class = classes[np.argmax(prediction)]
    confidence = np.max(prediction)
    return predicted_class, confidence

# Test with custom examples
sample_tweets = [
    "I am so happy today!",
    "This is terrible and makes me angry",
    "I'm feeling very sad right now",
    "I love spending time with my family",
    "That's really surprising!",
    "I'm scared about what might happen"
]

print("\n" + "="*60)
print("Sample Predictions:")
print("="*60)
for tweet in sample_tweets:
    sentiment, confidence = predict_sentiment(tweet)
    print(f"\nTweet: {tweet}")
    print(f"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})")

# Save the model
model.save('emotion_sentiment_model.h5')
print("Model saved as 'emotion_sentiment_model.h5'")

# Save tokenizer
import pickle
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)
print("Tokenizer saved as 'tokenizer.pkl'")